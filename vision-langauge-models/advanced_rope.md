---
title: 从原始 RoPE 到 YaRN 方法：高频与长序列的折衷
author: Yifeng Huang
date: 2025-01-19
---

# 1. 背景：原始 RoPE

## 1.1 Rotary Position Embedding 的核心

- **目的**：在 `Transformer` 中，通过“旋转”的方式给 Query/Key 向量注入位置信息，让注意力打分对相对位置 \((m-n)\) 更敏感。  
- **方法**：将隐藏向量按 2D 子空间划分 (或复数实部+虚部) 后，对每个维度施加一个基于位置的旋转角 \(\theta = \mathrm{inv\_freq}[i] \times m\)。  
  - **不同维度**有不同“频率”（\(\mathrm{inv\_freq}[i]\))，即有的转得快（高频）、有的转得慢（低频）。  
- **效果**：  
  - **高频维度**绕多圈，对短距离变化非常敏感；  
  - **低频维度**没绕满一圈，保留更多“绝对”位置特征，适合区分远距离位置。

## 1.2 面临的问题：长序列扩展

- 原始预训练的 RoPE 只见过最多 `L` tokens（如 2048）。当推理时想用更长序列（2K→8K→16K），RoPE 的相位超出了训练分布，模型性能常会显著退化。  
- **关键**：如果直接“硬用”原频率处理 >`L` 的位置，模型往往学不到有效的分布；若简单**统一缩放**又会破坏原本的高频维度，使得短程精细度丢失。

---

# 2. Positional Interpolation (PI)

## 2.1 核心思路

- 为扩展至 `s × L` 的上下文长度，用一个统一的缩放因子 `s`：将位置索引 `m` 变成 `m/s`（或类似公式），在计算 \(\theta_{m,i}\) 时除以 `s`。  
- 这样使得当 `m` 高达 `s×L` 时，其相位与原先在 `[0, L]` 内大致“对应”，尽量让模型对更长位置也能对齐到已学到的分布。

## 2.2 问题：高频抹除

- **统一缩放**把所有高频也“降频”了：  
  - 原本绕多圈的维度，现在只能绕几分之一圈，近距离分辨率被严重破坏；  
  - 实验中若 `s` 较大（> 8），质量显著下滑。

---

# 3. NTK-aware Interpolation

## 3.1 缘起与做法

- 引入神经网络的 **NTK 理论**，指出缺少高频成分时网络难捕捉局部细节。  
- **NTK-aware**：不是让所有维度都统一缩放，而通过调整 `base` 或相位公式，使得**高频维度少缩放**、低频维度多缩放。  
  - 典型做法：手动改变 RoPE 的 `base` 值，保留更多高频。

## 3.2 局限

- 需要**经验地选择一个合适的“调整后 base”**；若想改变上下文扩展倍数 `s`，往往要再次微调。  
- 但它已实证比单纯 PI 更好地保留了对近距离的敏感度。

---

# 4. NTK-by-parts Interpolation

## 4.1 进一步细分

- 论文指出：  
  - **多圈 (高频) 维度**等同于“相对信息”；  
  - **不满一圈 (低频) 维度**则更像“绝对位置”提示。  
- 在扩展到更大上下文时：  
  - 低频维度**必须**插值，否则在新大区间“没见过”；  
  - 高频维度**最好少动**，以免丢失短距分辨率；  
  - 中间维度可以做**线性过渡**。

## 4.2 效果

- 称为 **NTK-by-parts**：  
  - 对“旋转数” \(r(d) = \frac{L}{\lambda_d}\) 建立阈值 \(\alpha, \beta\)，在维度轴分成三段：不插值、完全插值、线性插值。  
- 相比 NTK-aware 更透明，更好兼顾长序列与短序列。

---

# 5. Dynamic Scaling (Dynamic NTK)

## 5.1 场景

- 在自回归推理中，序列会从 1 慢慢增长到最高长度。有些实现若固定一个 `s`，当序列尚未到 `s×L` 时就会出现浪费或性能折损。  
- 如果超过了 `s×L`，则又直接失效。

## 5.2 动态做法

- **Dynamic Scaling**：每次前向都把 `s = max(1, \tfrac{l'}{L})`，`l'` 为当前序列实际长度。  
  - 未超过 `L` 时 `s=1`；  
  - 超过时逐渐增大 `s`，平滑过渡。  
- 配合 NTK-aware 或 by-parts 即 **“Dynamic NTK”**，在推理中更优雅地处理任意序列长度。  
- 要注意 kv-cache 里如何存储 RoPE 数据，避免多次 forward 时不一致。

---

# 6. YaRN 方法

## 6.1 核心思路

- **综合**了前述见解，既有“NTK-by-parts”维度分段插值，又加上**Attention 温度**(或 length scaling) 的技巧。  
- 在实践中：  
  1. 提供两套频率：原频率 (`freq_extra`) + 缩放频率 (`freq_inter`)，再根据维度做线性插值；  
  2. 为 Query/Key 的正余弦乘上一个温度系数 `_mscale`，相当于修改 `softmax(q·k / t)` 而不改核心注意力代码；  
  3. 通过 `beta_fast`, `beta_slow` 等参数确定哪些维度保持高频、哪些做低频扩展或混合。

## 6.2 代码片段（简述）

```python
freq_extra = 1 / (base^(i/dim))          # 原始频率
freq_inter = 1 / (scaling_factor * base^(i/dim)) # 缩放后更低频
# ...
low, high = yarn_find_correction_range(beta_fast, beta_slow, ...)
inv_freq_mask = 1.0 - yarn_linear_ramp_mask(low, high, dim//2)
inv_freq = freq_inter*(1 - inv_freq_mask) + freq_extra*(inv_freq_mask)
# -> 在维度[0..low]更多用 freq_extra, [high..dim]更多 freq_inter
# -> 中间插值
